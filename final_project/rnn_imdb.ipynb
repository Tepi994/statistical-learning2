{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Movie Review Dataset\n",
    "\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. \n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rnn/trainIMDB.tsv', delimiter =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As preprocessing, it is necessary to remove certain characters that may cause noise during the training of the recurrent neural network. Punctuations marks are removed and all strings are set to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that normalizes the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_norm(text):\n",
    "    text = text.str.lower()\n",
    "    text = text.replace(r'[^\\w\\s]','')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace(to_replace =r'!', value='',regex=True)\n",
    "    text = text.replace('á', 'a')\n",
    "    text = text.replace('é', 'e')\n",
    "    text = text.replace('í', 'i')\n",
    "    text = text.replace('ó', 'o')\n",
    "    text = text.replace('ú', 'u')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace(to_replace =r'-', value='',regex=True)\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace('/', '')\n",
    "    text = text.replace('\\\\', '')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_clean'] = word_norm(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>\\the classic war of the worlds\\\" by timothy hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>the film starts with a manager (nicholas bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        review_clean  \n",
       "0  with all this stuff going down at the moment w...  \n",
       "1  \\the classic war of the worlds\\\" by timothy hi...  \n",
       "2  the film starts with a manager (nicholas bell)...  \n",
       "3  it must be assumed that those who praised this...  \n",
       "4  superbly trashy and wondrously unpretentious 8...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class allows to vectorize a text corpus, by turning each text into either a sequence of integers \n",
    "#(each integer being the index of a token in a dictionary)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Pads sequences to the same length.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dictionary\n",
    "\n",
    "We will define a vocabulary of the words present in the dataset. Each word will be associated to a number. This number is the feature representation of the word that we will use in our model. For this project, we will use the top 6000 most common words present in the dataset.\n",
    "\n",
    "At the moment, we have not removed stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our RNNs will be defined as Many-to-one\n",
    "\n",
    "This is due to the fact that we are performing sentiment analysis, we will have several input features (words) and we will return the probability of the review being positive or negative as an output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000 ##the maximum number of words to keep.\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "##Updates internal vocabulary based on a list of texts.\n",
    "tokenizer.fit_on_texts(df['review_clean'])\n",
    "tokens = tokenizer.texts_to_sequences(df['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>\\the classic war of the worlds\\\" by timothy hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>the film starts with a manager (nicholas bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        review_clean  \n",
       "0  with all this stuff going down at the moment w...  \n",
       "1  \\the classic war of the worlds\\\" by timothy hi...  \n",
       "2  the film starts with a manager (nicholas bell)...  \n",
       "3  it must be assumed that those who praised this...  \n",
       "4  superbly trashy and wondrously unpretentious 8...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorical max padding. \n",
    "\n",
    "input vectors for training all have to be of the same size. One way of defining the size or the padding required could be using as a benchmark the longest review of the dataset. This review has a total of 2470 words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2470"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['review_clean'].str.split().str.len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical pad size\n",
    "\n",
    "As first approximation, a pad of size 140 will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 140\n",
    "X_data = pad_sequences(tokens, maxlen=pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3,   51,    9, ...,   21,    1, 1563],\n",
       "       [   3,   52,  437, ...,   27,   90, 5537],\n",
       "       [   1, 1445, 1360, ...,  864, 1351,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    7,  358,  159],\n",
       "       [  10,  131,   11, ...,   16,   82,   80],\n",
       "       [   4,   55,   83, ...,   14,    3,  520]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df_train['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting the datsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
    "                                                    stratify=y_data, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: LSTM \n",
    "\n",
    "\n",
    "* Input vectors of size 128 for each word.\n",
    "* Long Term Short Memory Neural Net using with size 100 as an output.\n",
    "* Dense layer using sigmoid activation.\n",
    "* adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ModelCheckpoint \n",
    "\n",
    "* ModelCheckpoint will save the best model based on validation loss while training. \n",
    "* EarlyStopping will stop training when validation loss is no longer decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks_1 = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(('rnn/experiment_1/model.h5'), save_best_only=True, \n",
    "                             save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, None, 128)         768000    \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 859,701\n",
      "Trainable params: 859,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model_1 = tf.keras.Sequential()\n",
    "model_1.add(layers.Embedding(max_features,embed_size))\n",
    "model_1.add(layers.LSTM(100))\n",
    "model_1.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "print(model_1.summary())\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/6\n",
      "16000/16000 [==============================] - 172s 11ms/sample - loss: 0.4617 - acc: 0.7721 - val_loss: 0.3668 - val_acc: 0.8508\n",
      "Epoch 2/6\n",
      "16000/16000 [==============================] - 187s 12ms/sample - loss: 0.2697 - acc: 0.8926 - val_loss: 0.3227 - val_acc: 0.8665\n",
      "Epoch 3/6\n",
      "16000/16000 [==============================] - 186s 12ms/sample - loss: 0.2192 - acc: 0.9165 - val_loss: 0.3375 - val_acc: 0.8562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199a29f63c8>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 6\n",
    "model_1.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks =callbacks_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "Experiment 1 was first tested with a padding of 2470. This number represents the amount of words in the largest review of the dataset. However, each epoch took about 2 hours to train, so this experiment was discarded. \n",
    "\n",
    "<img src=\"rnn/maxpad.jfif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 5s 942us/sample - loss: 0.3322 - acc: 0.8568s - loss: 0.3\n"
     ]
    }
   ],
   "source": [
    "score_1 = model_1.evaluate(X_test, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1.1: LSTM \n",
    "\n",
    "\n",
    "* Input vectors of size 256 for each word.\n",
    "* Long Term Short Memory Neural Net using with size 100 as an output.\n",
    "* Dense layer using sigmoid activation.\n",
    "* adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, None, 256)         1536000   \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,678,901\n",
      "Trainable params: 1,678,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "callbacks_1_1 = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(('rnn/experiment_1/model_1_1.h5'), save_best_only=True, \n",
    "                             save_weights_only=False)]\n",
    "model_1_1 = tf.keras.Sequential()\n",
    "model_1_1.add(layers.Embedding(max_features,256))\n",
    "model_1_1.add(layers.LSTM(100))\n",
    "model_1_1.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "print(model_1_1.summary())\n",
    "model_1_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 283s 18ms/sample - loss: 0.4388 - acc: 0.7921 - val_loss: 0.3299 - val_acc: 0.8645\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 281s 18ms/sample - loss: 0.2611 - acc: 0.8941 - val_loss: 0.3402 - val_acc: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19a39b19668>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_1.fit(X_train,y_train, batch_size=batch_size, epochs=10, validation_split=0.2, callbacks =callbacks_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.3237 - acc: 0.8608\n"
     ]
    }
   ],
   "source": [
    "score_1_1 = model_1_1.evaluate(X_test, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(('rnn/experiment_0/model.h5'), save_best_only=True, \n",
    "                             save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/6\n",
      "16000/16000 [==============================] - 59s 4ms/sample - loss: 0.5159 - acc: 0.7477 - val_loss: 0.3433 - val_acc: 0.8540\n",
      "Epoch 2/6\n",
      "16000/16000 [==============================] - 52s 3ms/sample - loss: 0.2796 - acc: 0.8894 - val_loss: 0.3234 - val_acc: 0.8655\n",
      "Epoch 3/6\n",
      "16000/16000 [==============================] - 52s 3ms/sample - loss: 0.1997 - acc: 0.9270 - val_loss: 0.3530 - val_acc: 0.8673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19a3c1eb9b0>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(max_features, embed_size))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32, return_sequences = True)))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.05))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 6\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks =callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model stopped at epoch 4 because the validation accuracy of the training set did not improve on that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 4s 715us/sample - loss: 0.3417 - acc: 0.8688\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
